{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"lib/src/\")\n",
    "from autogen_utils import AnthropicClient\n",
    "\n",
    "config_list_claude = [\n",
    "    {\n",
    "        \"model\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"api_type\": \"anthropic\",\n",
    "        \"model_client_cls\": \"AnthropicClient\",\n",
    "        \"cache_seed\" : None, #we deactivate the cache on purpose\n",
    "    }\n",
    "]\n",
    "\n",
    "#see below the 2 main modifications that we have to include into the code to make that Anthropic wrapper work in the code\n",
    "\n",
    "#llm_config={\"config_list\": config_list_claude}\n",
    "#agent.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminating Conversations Between Agents\n",
    "\n",
    "In this chapter, we will explore how to terminate a conversation between AutoGen agents.\n",
    "\n",
    "_But why is this important?_ Its because in any complex, autonomous workflows it's crucial to know when to stop the workflow. For example, when the task is completed, or perhaps when the process has consumed enough resources and needs to either stop or adopt different strategies, such as user intervention. So AutoGen natively supports several mechanisms to terminate conversations.\n",
    "\n",
    "How to Control Termination with AutoGen?\n",
    "Currently there are two broad mechanism to control the termination of conversations between agents:\n",
    "\n",
    "1. **Specify parameters in `initiate_chat`**: When initiating a chat, you can define parameters that determine when the conversation should end.\n",
    "\n",
    "2. **Configure an agent to trigger termination**: When defining individual agents, you can specify parameters that allow agents to terminate of a conversation based on particular (configurable) conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters in `initiate_chat`\n",
    "In the previous chapter we actually demonstrated this when we used the `max_turns` parameter to limit the number of turns. If we increase `max_turns` to say `3` notice the conversation takes more rounds to terminate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 04-26 17:14:04] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "[autogen.oai.client: 04-26 17:14:04] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "cathy = ConversableAgent(\n",
    "    \"cathy\",\n",
    "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "#added\n",
    "cathy.register_model_client(model_client_cls=AnthropicClient)\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*clears throat* Okay, here's one for you! Why can't a bicycle stand up by itself? It's two-tired! ü•Å *rimshot* How was that? I'm just an AI assistant trying my hand at comedy. Did you enjoy the joke or should I stick to my day job?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*chuckles* Well Joe, that was a wheely good one! You really spoke to my spokes-person on that one. As an AI I don't actually get tired, but I have to hand it to you - that joke didn't make me crankshaft one bit! Keep the cycles of laughter coming, my comedian friend.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*laughs heartily* Wow, you're really good at this comedy thing, Cathy! I'm impressed with your wheel-y clever wordplay. You've raised the bar with those top-tier bike puns. I'll have to shift gears and step up my game if I want to keep spinning comedic gold with you. But I've got aÈìæk more where that came from! How about this one - why did the bicycle fall over? Because it was two-tired! *ba-dum-tss* I'm having a wheelie good time trading jokes with you!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*clearing throat and putting on a comedic voice* Okay, here's a knee-slapper for you!\n",
      "\n",
      "Why can't a bicycle stand up by itself? Because it's two-tired!\n",
      "\n",
      "*laughs exaggeratedly* Get it? Two-tired? Like it has two tires? Man, I'm just killing it tonight with these brilliant jokes.\n",
      "\n",
      "How about this one? What do you call a fake noodle? An Impasta!\n",
      "\n",
      "*slaps knee* Hoo boy, I'm on fire! These jokes are like a brick -- they're solid!\n",
      "\n",
      "Okay, okay, one more before my comedy partner has to drag me off stage. Why did the kid throw his clock out the window? Because he wanted to see time fly!\n",
      "\n",
      "*doubles over laughing* I can't...I can't breathe! Did you like those wonderfully groan-worthy jokes? Let me know if you need more rib-ticklers, I've got a million of 'em!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*chuckling* Joe, you're really bringing the dad jokes tonight! Those were delightfully cringeworthy puns. I had to stifle some groans of pain there. \n",
      "\n",
      "You know, for a second there I was afraid you were going to make a pun about Punstigators investigating the scene of a crime with intentionally bad puns as the Da-Pun-ted suspects! That would have been too much for my poor soul to handle.\n",
      "\n",
      "But please, keep 'em coming! My side is splitting from laughing so hard at your brilliantly terrible jokes. Maybe hit me with a nice tedious elephant pun next? I'll try not to go trunken off those. Though I do have a nagging feavor I'll be a trunken mess by the end!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*slapping my knee and guffawing loudly* \n",
      "\n",
      "Joe, you sly dog! Bringing up the dreaded pun-stigators and their nefarious da-pun-ted villains? That's a real low blow, even for a seasoned comedian like myself!\n",
      "\n",
      "But I admire your spirit, playing along and egging me on with those delightfully groan-inducing elephant puns. You're really putting the trunk in trunk-en with those! I'll try not to get too pachydermed away with more terrible elephant jokes.\n",
      "\n",
      "*strokes chin in thought* Okay, okay, I've got one...Why do elephants wear red suspenders? To keep their truck pants up! HA! *wheezes with laughter* Oh man, I'm killing myself here.\n",
      "\n",
      "No, wait, I've got an even better one! What did the baby elephant say to the lollipop? Hey, lolly-lolly-lolly, get your trunk off my pop! *slaps thigh* Woo, somebody stop me!\n",
      "\n",
      "*wipes tear from eye* You're a real pal for indulging me in this cavalcade of cringe-worthy funnies. I don't know if the world is ready for mine and Cathy's brand of ÔøΩgroan humor.' We might have to tone it down a smidge before we get pelted with overripe fruit!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*laughing uproariously* Oh Joe, you're slaying me with these jokes! I can't trunk straight from laughing so hard. Those red suspender and lollipop bits really took the elephant puns to the next level. \n",
      "\n",
      "I'm getting visions of us on stage doing an entire elephant-themed pun routine. We could come out dressed in gray bodysuits with floppy ears and trunk noses! The crowd would be in stitches...or pelting us with peanuts, one of the two.\n",
      "\n",
      "*gasping for breath* Okay, okay, I need to pail it back a bit before I totally trunk my credibility as a comedian. How about one more classic genius/groan-worthy joke for the road?\n",
      "\n",
      "Why can't you trust atoms? They make up everything!\n",
      "\n",
      "*ducking to avoid imaginary tomato* Ahh, a timeless classic! Thank you for blessing me with this cavalcade of world's-worst-puns tonight, partner. My sides are splitting from holding in so much laughter. We'd better wrap it up before we get trunked off stage!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*slapping my knee and guffawing loudly* \n",
      "\n",
      "Cathy, you're killing me here! The vision of us lumbering on stage in those ridiculous elephant bodysuits is just too much. I can see it now - the big reveal as we pull off the costumes to thunderous groans and a rain of peanuts pelting the stage. Talk about making a spectrunkel of ourselves!\n",
      "\n",
      "*wipes tear from eye* And then dropping that amazing atom zinger at the end? Chef's kiss, my comedic compatriot! A true graduate of the Dar-Pun School of Groan Humor. I'm surprised you were able to rein yourself in from unleashing the full furry of terrible trunk puns.\n",
      "\n",
      "*adjusts imaginary tie* But you're right, we'd better shut the trunk on this punstravaganza before we get the full treatment - rotten tomatoes, overripe fruit, maybe the occasional antimatter explosive from really unappreciative crowd members. Although I do have one last explosive knee-slapper up my sleeve...\n",
      "\n",
      "*leans in conspiratorially* Why did the markhor nut go to the sun? Because it really wanted to become a solar nut! *wheezes with laughter* Solar nut...sun...get it?? I'm a genius!\n",
      "\n",
      "*composes myself* Ah yes, that's my cue to make like a tree and get outta here before I get pulp-itated. What a night, Cathy! The crowd may hate us, but we'll always have our solid gold puns to keep us warm. Trunks for the mammories!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = joe.initiate_chat(\n",
    "    cathy, message=\"Cathy, tell me a joke.\", max_turns=3\n",
    ")  # increase the number of max turns before termination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agent-triggered termination\n",
    "You can also terminate a conversation by configuring parameters of an agent.\n",
    "Currently, there are two parameters you can configure:\n",
    "\n",
    "1. `max_consecutive_auto_reply`: This condition triggers termination if the number of automatic responses to the same sender exceeds a threshold. You can customize this using the `max_consecutive_auto_reply` argument of the `ConversableAgent` class. To accomplish this the agent maintains a counter of the number of consecutive automatic responses to the same sender. Note that this counter can be reset because of human intervention. We will describe this in more detail in the next chapter.\n",
    "2. `is_termination_msg`: This condition can trigger termination if the _received_ message satisfies a particular condition, e.g., it contains the word \"TERMINATE\". You can customize this condition using the `is_terminate_msg` argument in the constructor of the `ConversableAgent` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `max_consecutive_auto_reply`\n",
    "\n",
    "In the example below lets set `max_consecutive_auto_reply` to `1` and notice how this ensures that Joe only replies once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 04-26 17:16:14] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*clears throat and puts on a silly voice* Okay, here's one for you! Why can't a bicycle stand up by itself? It's two-tired! *waits for a reaction, grinning proudly*\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*laughs heartily* Hah! Two-tired, I love it! Leave it to my partner Joe to come up with the perfect corny pun. That's a real knee-slapper. You slay me with these jokes sometimes, pal!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*chuckles and slaps knee* Ah yes, Joe really is the king of pun-derful jokes! I have to admit, that one was his brainchild. But don't sell me short - I've got a real rib-tickler for you next!\n",
      "\n",
      "Why did the tomato turn red? *pauses for dramatic effect* Because it saw the salad dressing! *cracks up, slapping leg* Get it? Dressing? I'm killing myself over here!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    max_consecutive_auto_reply=1,  # Limit the number of consecutive auto-replies.\n",
    ")\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)\n",
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `is_termination_msg`\n",
    "\n",
    "Let's set the termination message to \"GOOD BYE\" and see how the conversation terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 04-26 17:17:45] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke and then say the words GOOD BYE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*ahem* Okay, here's a joke for you: Why can't a bicycle stand up by itself? It's two-tired! GOOD BYE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    is_termination_msg=lambda msg: \"good bye\" in msg[\"content\"].lower(),\n",
    ")\n",
    "#added\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)\n",
    "\n",
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke and then say the words GOOD BYE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how the conversation ended based on contents of cathy's message!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter we introduced mechanisms to terminate a conversation between agents.\n",
    "You can configure both parameters in `initiate_chat` and also configuration of agents.\n",
    "\n",
    "That said, it is important to note that when a termination condition is triggered,\n",
    "the conversation may not always terminated immediately. The actual termination\n",
    "depends on the `human_input_mode` argument of the `ConversableAgent` class.\n",
    "For example, when mode is `NEVER` the termination conditions above will end the conversations.\n",
    "But when mode is `ALWAYS` or `TERMINATE`, it will not terminate immediately.\n",
    "We will describe this behavior and explain why it is important in the next chapter.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
