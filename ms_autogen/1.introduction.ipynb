{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen==0.2.27 (from -r requirements.txt (line 1))\n",
      "  Using cached pyautogen-0.2.27-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting anthropic>=0.23.1 (from -r requirements.txt (line 2))\n",
      "  Using cached anthropic-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting diskcache (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyautogen==0.2.27->-r requirements.txt (line 1)) (6.1.3)\n",
      "Collecting flaml (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached FLAML-2.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyautogen==0.2.27->-r requirements.txt (line 1)) (1.22.4)\n",
      "Collecting openai>=1.3 (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached openai-1.30.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pydantic!=2.6.0,<3,>=1.10 (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Collecting python-dotenv (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting termcolor (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.23.1->-r requirements.txt (line 2)) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic>=0.23.1->-r requirements.txt (line 2))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.23.1->-r requirements.txt (line 2)) (0.27.0)\n",
      "Collecting jiter<1,>=0.1.0 (from anthropic>=0.23.1->-r requirements.txt (line 2))\n",
      "  Using cached jiter-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.23.1->-r requirements.txt (line 2)) (1.3.1)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic>=0.23.1->-r requirements.txt (line 2))\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.23.1->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.3->pyautogen==0.2.27->-r requirements.txt (line 1)) (4.66.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=2.6.0,<3,>=1.10->pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic!=2.6.0,<3,>=1.10->pyautogen==0.2.27->-r requirements.txt (line 1))\n",
      "  Using cached pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.0->anthropic>=0.23.1->-r requirements.txt (line 2))\n",
      "  Using cached huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker->pyautogen==0.2.27->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker->pyautogen==0.2.27->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker->pyautogen==0.2.27->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker->pyautogen==0.2.27->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken->pyautogen==0.2.27->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic>=0.23.1->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=14.0->docker->pyautogen==0.2.27->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->docker->pyautogen==0.2.27->-r requirements.txt (line 1)) (3.3.2)\n",
      "Using cached pyautogen-0.2.27-py3-none-any.whl (273 kB)\n",
      "Using cached anthropic-0.26.1-py3-none-any.whl (877 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "Using cached openai-1.30.3-py3-none-any.whl (320 kB)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n",
      "Installing collected packages: termcolor, python-dotenv, pydantic-core, jiter, flaml, distro, diskcache, annotated-types, tiktoken, pydantic, huggingface-hub, tokenizers, openai, pyautogen, anthropic\n",
      "Successfully installed annotated-types-0.7.0 anthropic-0.26.1 diskcache-5.6.3 distro-1.9.0 flaml-2.1.2 huggingface-hub-0.23.1 jiter-0.4.0 openai-1.30.3 pyautogen-0.2.27 pydantic-2.7.1 pydantic-core-2.18.2 python-dotenv-1.0.1 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"lib/src/\")\n",
    "from autogen_utils import AnthropicClient\n",
    "\n",
    "config_list_claude = [\n",
    "    {\n",
    "        \"model\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"api_type\": \"anthropic\",\n",
    "        \"model_client_cls\": \"AnthropicClient\",\n",
    "        \"cache_seed\" : None, #we deactivate the cache on purpose\n",
    "    }\n",
    "]\n",
    "\n",
    "#see below the 2 main modifications that we have to include into the code to make that Anthropic wrapper work in the code\n",
    "\n",
    "#llm_config={\"config_list\": config_list_claude}\n",
    "#agent.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AutoGen\n",
    "\n",
    "Welcome! AutoGen is an open-source framework that leverages multiple _agents_ to enable complex workflows. This tutorial introduces basic concepts and building blocks of AutoGen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why AutoGen?\n",
    "\n",
    "> _The whole is greater than the sum of its parts._<br/>\n",
    "> -**Aristotle**\n",
    "\n",
    "While there are many definitions of agents, in AutoGen, an agent is an entity that can send messages, receive messages and generate a reply using models, tools, human inputs or a mixture of them.\n",
    "This abstraction not only allows agents to model real-world and abstract entities, such as people and algorithms, but it also simplifies implementation of complex workflows as collaboration among agents.\n",
    "\n",
    "Further, AutoGen is extensible and composable: you can extend a simple agent with customizable components and create workflows that can combine these agents and power a more sophisticated agent, resulting in implementations that are modular and easy to maintain.\n",
    "\n",
    "Most importantly, AutoGen is developed by a vibrant community of researchers\n",
    "and engineers. It incorporates the latest research in multi-agent systems\n",
    "and has been used in many real-world applications, including agent platform,\n",
    "advertising, AI employees, blog/article writing, blockchain, calculate burned areas by wildfires,\n",
    "customer support, cybersecurity, data analytics, debate, education, finance, gaming, legal consultation,\n",
    "research, robotics, sales/marketing, social simulation, software engineering,\n",
    "software security, supply chain, t-shirt design, training data generation, Youtube service..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "The simplest way to install AutoGen is from pip: `pip install pyautogen`. Find more options in [Installation](/docs/installation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "In AutoGen, an agent is an entity that can send and receive messages to and from\n",
    "other agents in its environment. An agent can be powered by models (such as a large language model\n",
    "like GPT-4), code executors (such as an IPython kernel), human, or a combination of these\n",
    "and other pluggable and customizable components.\n",
    "\n",
    "```{=mdx}\n",
    "![ConversableAgent](./assets/conversable-agent.jpg)\n",
    "```\n",
    "\n",
    "An example of such agents is the built-in `ConversableAgent` which supports the following components:\n",
    "\n",
    "1. A list of LLMs\n",
    "2. A code executor\n",
    "3. A function and tool executor\n",
    "4. A component for keeping human-in-the-loop\n",
    "\n",
    "You can switch each component on or off and customize it to suit the need of \n",
    "your application. For advanced users, you can add additional components to the agent\n",
    "by using [`registered_reply`](../reference/agentchat/conversable_agent/#register_reply). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs, for example, enable agents to converse in natural languages and transform between structured and unstructured text. \n",
    "The following example shows a `ConversableAgent` with a GPT-4 LLM switched on and other\n",
    "components switched off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 05-27 07:06:57] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "    function_map=None,  # No registered functions, by default it is None.\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "#added\n",
    "agent.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `llm_config` argument contains a list of configurations for the LLMs.\n",
    "See [LLM Configuration](/docs/topics/llm_configuration) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask this agent to generate a response to a question using the `generate_reply` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a silly joke for you:\n",
      "\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n"
     ]
    }
   ],
   "source": [
    "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roles and Conversations\n",
    "\n",
    "In AutoGen, you can assign roles to agents and have them participate in conversations or chat with each other. A conversation is a sequence of messages exchanged between agents. You can then use these conversations to make progress on a task. For example, in the example below, we assign different roles to two agents by setting their\n",
    "`system_message`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 05-27 07:07:35] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "[autogen.oai.client: 05-27 07:07:35] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "cathy = ConversableAgent(\n",
    "    \"cathy\",\n",
    "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 05-27 07:09:47] {465} WARNING - Model client AnthropicClient is already registered. Add more entries in the config_list to use multiple model clients.\n",
      "[autogen.oai.client: 05-27 07:09:47] {465} WARNING - Model client AnthropicClient is already registered. Add more entries in the config_list to use multiple model clients.\n"
     ]
    }
   ],
   "source": [
    "#added\n",
    "cathy.register_model_client(model_client_cls=AnthropicClient)\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two comedian agents, we can ask them to start a comedy show.\n",
    "This can be done using the `initiate_chat` method.\n",
    "We set the `max_turns` to 2 to keep the conversation short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*straightens bowtie and clears throat* Okay, here's one for you! Why can't a bicycle stand up by itself? It's two-tired! *waits for laughter, drums on knee* Ah, come on, that was a classic! My comedy partner Pat is usually better at this banter thing than me. Want to hear another?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*chuckles* Not bad, not bad! Though I've heard that one wheely too many times before. But hey, I'm always game for more jokes from the comedic genius of Joe and Pat. Hit me with another zinger!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*slicks back hair and puffs out chest confidently* Alright, alright, you asked for it! Why do seagulls fly over the sea? Because if they flew over the bay, they would be bagels! *waits for rimshot, looks around awkwardly* ...Too far? My bad, I'm still working on my material. \n",
      "\n",
      "*regains composure* But listen, this one will really knock your socks off: A courtroom artist was arrested today for an unknown reason... details are sketchy! *holds for laughter, winks* That's what I'm talking about - high brow humor for a classy audience like yourself!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comedians are bouncing off each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we introduced the concept of agents, roles and conversations in AutoGen.\n",
    "For simplicity, we only used LLMs and created fully autonomous agents (`human_input_mode` was set to `NEVER`). \n",
    "In the next chapter, \n",
    "we will show how you can control when to _terminate_ a conversation between autonomous agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
