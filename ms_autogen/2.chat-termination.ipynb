{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"lib/src/\")\n",
    "from autogen_utils import AnthropicClient\n",
    "\n",
    "config_list_claude = [\n",
    "    {\n",
    "        \"model\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"api_type\": \"anthropic\",\n",
    "        \"model_client_cls\": \"AnthropicClient\",\n",
    "        \"cache_seed\" : None, #we deactivate the cache on purpose\n",
    "    }\n",
    "]\n",
    "\n",
    "#see below the 2 main modifications that we have to include into the code to make that Anthropic wrapper work in the code\n",
    "\n",
    "#llm_config={\"config_list\": config_list_claude}\n",
    "#agent.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminating Conversations Between Agents\n",
    "\n",
    "In this chapter, we will explore how to terminate a conversation between AutoGen agents.\n",
    "\n",
    "_But why is this important?_ Its because in any complex, autonomous workflows it's crucial to know when to stop the workflow. For example, when the task is completed, or perhaps when the process has consumed enough resources and needs to either stop or adopt different strategies, such as user intervention. So AutoGen natively supports several mechanisms to terminate conversations.\n",
    "\n",
    "How to Control Termination with AutoGen?\n",
    "Currently there are two broad mechanism to control the termination of conversations between agents:\n",
    "\n",
    "1. **Specify parameters in `initiate_chat`**: When initiating a chat, you can define parameters that determine when the conversation should end.\n",
    "\n",
    "2. **Configure an agent to trigger termination**: When defining individual agents, you can specify parameters that allow agents to terminate of a conversation based on particular (configurable) conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters in `initiate_chat`\n",
    "In the previous chapter we actually demonstrated this when we used the `max_turns` parameter to limit the number of turns. If we increase `max_turns` to say `3` notice the conversation takes more rounds to terminate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 05-27 07:14:20] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "[autogen.oai.client: 05-27 07:14:20] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "cathy = ConversableAgent(\n",
    "    \"cathy\",\n",
    "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "#added\n",
    "cathy.register_model_client(model_client_cls=AnthropicClient)\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*clears throat and puts on a goofy voice* Okay, okay! Why did the tomato turn red? Because it saw the salad dressing!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*laughs uproariously* Oh Joe, you really are a riot! That was a GREAT one! Ok, ok, let me try... *in a silly voice* Why was the math book sad? Because it had too many problems!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*laughs heartily in an over-the-top way* Hoo boy, Martha! That one really squared the circle on me! You're kracking me up over here! *wipes a tear from my eye* Okay, okay, let me hit you with another zinger. Why did the bicycle fall over? Because it was two-tired! *breaks into another fit of laughter, slapping my knee*\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*clears throat and puts on a comedic voice* Okay, here's a classic! Why can't a bicycle stand up by itself? Because it's two-tired! *pauses for laughter* Get it? Two-tired? Like too tired to stand up? Man, my humor really is re-cycled!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*chuckles* Not bad Joe, not bad! You really wheelie know how to spin a good pun. Though I tire of all these bicycle jokes. Maybe we should cycle through some new material? *laughs*\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*laughing* Cathy, you're absolutely right, we should brake from the cycling puns before we get too tired and spoked out! How about this one instead:\n",
      "\n",
      "Why can't a nose be 12 inches long? Because then it would be a foot! *waits for reaction with a goofy grin*\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*laughs heartily* Oh Joe, you really are a nosing around for the best jokes! That one was pun-tastic. But don't get too big for your britches - I've got a real zinger coming your way. \n",
      "\n",
      "Why can't you trust atoms? They make up everything! *pauses for effect* Get it? Make up everything? Because atoms are the building blocks of matter? *guffaws loudly* I slay me!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*slaps knee laughing* Cathy, you've got me in stitches over here! That was brilliant, an atom-ic bomb of a pun if I've ever heard one. You know, we make a great comedic duo - whenever I think we've reached our periodic stable of jokes, you come element-ing in with another zinger!\n",
      "\n",
      "*composes myself* Okay, okay, let me see if I can match your nuclear-level pun game. Why do seagulls fly over the sea? Because if they flew over the bay, they would be bagels! *waits for reaction with a cheesy grin*\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = joe.initiate_chat(\n",
    "    cathy, message=\"Cathy, tell me a joke.\", max_turns=3\n",
    ")  # increase the number of max turns before termination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agent-triggered termination\n",
    "You can also terminate a conversation by configuring parameters of an agent.\n",
    "Currently, there are two parameters you can configure:\n",
    "\n",
    "1. `max_consecutive_auto_reply`: This condition triggers termination if the number of automatic responses to the same sender exceeds a threshold. You can customize this using the `max_consecutive_auto_reply` argument of the `ConversableAgent` class. To accomplish this the agent maintains a counter of the number of consecutive automatic responses to the same sender. Note that this counter can be reset because of human intervention. We will describe this in more detail in the next chapter.\n",
    "2. `is_termination_msg`: This condition can trigger termination if the _received_ message satisfies a particular condition, e.g., it contains the word \"TERMINATE\". You can customize this condition using the `is_terminate_msg` argument in the constructor of the `ConversableAgent` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `max_consecutive_auto_reply`\n",
    "\n",
    "In the example below lets set `max_consecutive_auto_reply` to `1` and notice how this ensures that Joe only replies once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 05-27 07:15:14] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*clears throat and puts on a silly voice* Okay, here's a classic joke for you!\n",
      "\n",
      "Why can't a bicycle stand up by itself?\n",
      "Because it's two-tired!\n",
      "\n",
      "*giggles at my own joke* Get it? Too tired? Like it's tired from pedaling all day? Oh boy, I just crack myself up sometimes!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "*chuckles heartily* Ah, that's a good one Cathy! You always know how to brighten my day with your delightfully corny jokes. Two-tired, I love it! Leave it to you to take something so simple and make it into comedic gold. We're just the dynamic duo of dad jokes over here, aren't we? Keep 'em coming, I can take it!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*laughs uproariously in my goofy comedian persona* That's right, we're the reigning queens of the lame joke kingdom! I'm just getting warmed up though, here's another zinger for you:\n",
      "\n",
      "What kind of shoes do frogs wear? Open toad!\n",
      "\n",
      "*slaps my knee and guffaws* Woo, I'm on a roll today! You know you love these jokes, even if they're bad enough to make a whoopee cushion cringe. We've got all the time in the world for some high-brow low-humor, so hit me with another straight line if you've got one!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    max_consecutive_auto_reply=1,  # Limit the number of consecutive auto-replies.\n",
    ")\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)\n",
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `is_termination_msg`\n",
    "\n",
    "Let's set the termination message to \"GOOD BYE\" and see how the conversation terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 05-27 07:15:25] {426} INFO - Detected custom model client in config: AnthropicClient, model client can not be used until register_model_client is called.\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Cathy, tell me a joke and then say the words GOOD BYE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "*ahem* Okay, here's a classic:\n",
      "\n",
      "Why can't a bicycle stand up by itself? It's two-tired! GOOD BYE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": config_list_claude},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    is_termination_msg=lambda msg: \"good bye\" in msg[\"content\"].lower(),\n",
    ")\n",
    "#added\n",
    "joe.register_model_client(model_client_cls=AnthropicClient)\n",
    "\n",
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke and then say the words GOOD BYE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how the conversation ended based on contents of cathy's message!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter we introduced mechanisms to terminate a conversation between agents.\n",
    "You can configure both parameters in `initiate_chat` and also configuration of agents.\n",
    "\n",
    "That said, it is important to note that when a termination condition is triggered,\n",
    "the conversation may not always terminated immediately. The actual termination\n",
    "depends on the `human_input_mode` argument of the `ConversableAgent` class.\n",
    "For example, when mode is `NEVER` the termination conditions above will end the conversations.\n",
    "But when mode is `ALWAYS` or `TERMINATE`, it will not terminate immediately.\n",
    "We will describe this behavior and explain why it is important in the next chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
